{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of CNN: Grad-CAM\n",
    "* **Objective**: Convolutional Neural Networks are widely used on computer vision. It is powerful for processing grid-like data. However we hardly know how and why it works, due to the lack of decomposability into individually intuitive components. In this assignment, we use Grad-CAM, which highlights the regions of the input image that were important for the neural network prediction.\n",
    "\n",
    "* **To be submitted by next session**: this notebook, **cleaned** (i.e. without results, for file size reasons: `menu > kernel > restart and clean`), in a state ready to be executed (if one just presses 'Enter' till the end, one should obtain all the results for all images) with a few comments at the end. No additional report, just the notebook!\n",
    "\n",
    "* NB: if `PIL` is not installed, try `conda install pillow`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import  save_image\n",
    "import torch.nn as nn\n",
    "from torchvision import models, datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vgg_16.png](https://www.researchgate.net/profile/Bibo_Shi/publication/323440752/figure/fig1/AS:739814685032448@1553396974148/The-architecture-of-VGG-16-model-To-represent-different-depth-levels-convolutional.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI-PC\\.conda\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "# The downloading process may take a few minutes. \n",
    "# load the vgg-16 model trained on Animals10 dataset using transfer learning.\n",
    "# net = torch.load('animals10_resnet18_V1.pth', map_location=torch.device('cpu')) \n",
    "net = torch.load('animals10vgg_V0.pth', map_location=torch.device('cpu')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad-CAM \n",
    "* **Overview:** Given an image, and a category (‘tiger cat’) as input, we forward-propagate the image through the model to obtain the `raw class scores` before softmax. The gradients are set to zero for all classes except the desired class (tiger cat), which is set to 1. This signal is then backpropagated to the `rectified convolutional feature map` of interest, where we can compute the coarse Grad-CAM localization (blue heatmap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor():\n",
    "    \"\"\" Class for extracting activations and \n",
    "    registering gradients from targetted intermediate layers \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layers):\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.gradients = []\n",
    "\n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients.append(grad)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outputs = []\n",
    "        self.gradients = []\n",
    "        for name, module in self.model._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.target_layers:\n",
    "                x.register_hook(self.save_gradient)\n",
    "                outputs += [x]\n",
    "        return outputs, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelOutputs():\n",
    "    \"\"\" Class for making a forward pass, and getting:\n",
    "    __call__\n",
    "    1. The network output.\n",
    "    2. Activations from intermeddiate targetted layers.\n",
    "    get_gradient()\n",
    "    3. Gradients from intermeddiate targetted layers. \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layers):\n",
    "        self.model = model\n",
    "        self.feature_extractor = FeatureExtractor(self.model.features, target_layers)\n",
    "\n",
    "    def get_gradients(self):\n",
    "        return self.feature_extractor.gradients\n",
    "\n",
    "    def __call__(self, x):\n",
    "        target_activations, output = self.feature_extractor(x)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.model.classifier(output)\n",
    "        return target_activations, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCam:\n",
    "    def __init__(self, model, target_layer_names):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.extractor = ModelOutputs(self.model, target_layer_names)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "    def __call__(self, input, index=None):\n",
    "        \n",
    "        features, output = self.extractor(input)\n",
    "        \n",
    "        if index == None:\n",
    "            index = np.argmax(output.cpu().data.numpy())\n",
    "\n",
    "        # Set all output to be zero except the target class\n",
    "        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
    "        one_hot[0][index] = 1\n",
    "        one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
    "        one_hot = torch.sum(one_hot * output)\n",
    "\n",
    "        # Reset the gradient and perform backpropagation\n",
    "        self.model.features.zero_grad()\n",
    "        self.model.classifier.zero_grad()\n",
    "        one_hot.backward(retain_graph=True)\n",
    "        \n",
    "        # Get the gradients of target layer\n",
    "        grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
    "\n",
    "\n",
    "        target = features[-1]\n",
    "        target = target.cpu().data.numpy()[0, :]\n",
    "\n",
    "        # Calculate the weight of the feature (average of gradient) \n",
    "        weights = np.mean(grads_val, axis=(2, 3))[0, :]\n",
    "        cam = np.zeros(target.shape[1:], dtype=np.float32)\n",
    "\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * target[i, :, :]\n",
    "\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cv2.resize(cam, (224, 224))\n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / np.max(cam)\n",
    "        return cam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def show_cam_on_image(img, mask, file_name):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    cv2.imwrite(file_name, np.uint8(255 * cam))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Images and apply Grad-CAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_label = [0, 0, 1, 2, 1, 5, 5, 9]\n",
    "wrong_label = [5, 6, 6, 1, 0, 3, 9, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg25_0 (100).bmp\n",
      "5\n",
      "vgg25_0 (93).bmp\n",
      "6\n",
      "vgg25_1 (47).bmp\n",
      "6\n",
      "vgg25_2 (64).bmp\n",
      "1\n",
      "vgg25_4 (90).bmp\n",
      "0\n",
      "vgg25_5 (20).bmp\n",
      "3\n",
      "vgg25_5 (51).bmp\n",
      "9\n",
      "vgg25_9 (2).bmp\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# path = './data/ori/'\n",
    "# path = './data/resnet noise/'\n",
    "path = './data/vgg/'\n",
    "listing = os.listdir(path)\n",
    "\n",
    "grad_cam = GradCam(net, target_layer_names=[\"25\"])\n",
    "\n",
    "for image_name, target_label in zip(listing, right_label):\n",
    "    print(image_name)\n",
    "    im_orig_ori = Image.open(path + image_name)\n",
    "    im = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor()])(im_orig_ori)\n",
    "    im_orig_ori.close()\n",
    "    im = im[None, :, :, :]\n",
    "\n",
    "    label = torch.argmax(net.forward(torch.autograd.Variable(im, requires_grad=True)).data).item()\n",
    "    print (label)\n",
    "    im = im.requires_grad_(True)\n",
    "    # calculating the masks\n",
    "    mask = grad_cam(im, target_label)\n",
    "    # show cam on the image\n",
    "    show_cam_on_image(np.transpose(im.detach().squeeze().numpy(), (1, 2, 0)), mask, 'GC_'+image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforce the noise for a better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vggdiff_25_1 (47).bmp\n",
      "vggdiff_25_4 (90).bmp\n",
      "vggdiff_25_5 (51).bmp\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import ImageOps, ImageEnhance\n",
    "# path = './data/ori/'\n",
    "# path = './data/resnet noise/'\n",
    "path = './data/vgg noise/'\n",
    "listing = os.listdir(path)\n",
    "\n",
    "for image_name, target_label in zip(listing, wrong_label):\n",
    "    print(image_name)\n",
    "    im_orig_ori = Image.open(path + image_name)\n",
    "    inv = ImageOps.invert(im_orig_ori)\n",
    "    im = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor()])(inv)\n",
    "    im = im - im%im[0,0,0]\n",
    "    im_orig_ori.close()\n",
    "    save_image(im, image_name)\n",
    "#     torch im = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = './data/ori/'\n",
    "listing = os.listdir(path)\n",
    "\n",
    "for image_name, target_label in zip(listing, wrong_label):\n",
    "    im_orig_ori = Image.open(path + image_name)\n",
    "    im = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor()])(im_orig_ori)\n",
    "    im_orig_ori.close()\n",
    "    save_image(im, image_name)\n",
    "#     torch im = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-torch]",
   "language": "python",
   "name": "conda-env-.conda-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
